{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "892ddda0-91d0-40b8-9313-9ee8f383f78f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting Energy Consumption Analysis...\n",
      "‚úì Data loaded successfully from database view\n",
      "Dataset shape: (1826, 4)\n",
      "Columns: ['day', 'daily_avg_kwh', 'peak_kwh', 'total_solar']\n",
      "‚úì Time series preprocessed: 1826 observations\n",
      "Date range: 2018-01-01 00:00:00 to 2022-12-31 00:00:00\n",
      "‚úì Original series plot saved\n",
      "\n",
      "--- Seasonal Decomposition ---\n",
      "‚úì Seasonal decomposition plot saved\n",
      "\n",
      "=== STATIONARITY ANALYSIS ===\n",
      "\n",
      "--- Stationarity Check: Original Series ---\n",
      "Test Statistic           -42.791544\n",
      "p-value                    0.000000\n",
      "Lags Used                  0.000000\n",
      "Observations Used       1825.000000\n",
      "Critical Value (1%)       -3.433938\n",
      "Critical Value (5%)       -2.863125\n",
      "Critical Value (10%)      -2.567614\n",
      "Conclusion: ‚úì Stationary\n",
      "‚úì Original series is stationary\n",
      "\n",
      "--- ACF/PACF Analysis ---\n",
      "‚úì ACF/PACF plots saved\n",
      "\n",
      "--- Data Split ---\n",
      "Training set: 1460 observations\n",
      "Test set: 366 observations\n",
      "\n",
      "=== ARIMA MODEL FITTING ===\n",
      "Trying ARIMA(1,0,1)...\n",
      "  AIC: -2473.88\n",
      "Trying ARIMA(2,0,2)...\n",
      "  AIC: -2471.81\n",
      "Trying ARIMA(5,0,5)...\n",
      "  AIC: -2476.30\n",
      "Trying ARIMA(1,0,2)...\n",
      "  AIC: -2472.94\n",
      "Trying ARIMA(2,0,1)...\n",
      "  AIC: -2473.08\n",
      "\n",
      "‚úì Best model: ARIMA(5, 0, 5) (AIC: -2476.30)\n",
      "\n",
      "Model Summary:\n",
      "                               SARIMAX Results                                \n",
      "==============================================================================\n",
      "Dep. Variable:          daily_avg_kwh   No. Observations:                 1460\n",
      "Model:                 ARIMA(5, 0, 5)   Log Likelihood                1250.151\n",
      "Date:                Sat, 19 Jul 2025   AIC                          -2476.301\n",
      "Time:                        17:16:34   BIC                          -2412.867\n",
      "Sample:                    01-01-2018   HQIC                         -2452.638\n",
      "                         - 12-30-2021                                         \n",
      "Covariance Type:                  opg                                         \n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const          1.5043      0.003    560.400      0.000       1.499       1.510\n",
      "ar.L1          0.1192      0.963      0.124      0.901      -1.768       2.006\n",
      "ar.L2         -0.3232      0.842     -0.384      0.701      -1.974       1.327\n",
      "ar.L3         -0.0107      0.990     -0.011      0.991      -1.950       1.929\n",
      "ar.L4         -0.2388      0.755     -0.316      0.752      -1.719       1.242\n",
      "ar.L5         -0.6455      0.814     -0.793      0.428      -2.240       0.949\n",
      "ma.L1         -0.0985      0.954     -0.103      0.918      -1.968       1.771\n",
      "ma.L2          0.3309      0.818      0.405      0.686      -1.272       1.934\n",
      "ma.L3         -0.0307      0.977     -0.031      0.975      -1.945       1.884\n",
      "ma.L4          0.2080      0.789      0.264      0.792      -1.338       1.754\n",
      "ma.L5          0.6634      0.810      0.819      0.413      -0.924       2.251\n",
      "sigma2         0.0105      0.000     29.295      0.000       0.010       0.011\n",
      "===================================================================================\n",
      "Ljung-Box (L1) (Q):                   0.01   Jarque-Bera (JB):                12.66\n",
      "Prob(Q):                              0.90   Prob(JB):                         0.00\n",
      "Heteroskedasticity (H):               1.27   Skew:                             0.09\n",
      "Prob(H) (two-sided):                  0.01   Kurtosis:                         3.42\n",
      "===================================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Covariance matrix calculated using the outer product of gradients (complex-step).\n",
      "\n",
      "--- Model Diagnostics ---\n",
      "‚úì Ljung-Box test: No significant autocorrelation in residuals\n",
      "‚ö†Ô∏è  Jarque-Bera test: Residuals may not be normally distributed\n",
      "\n",
      "=== MODEL EVALUATION ===\n",
      "Test RMSE: 0.1042\n",
      "Test MAE: 0.0814\n",
      "Test R¬≤: -0.0027\n",
      "Test MAPE: 5.43%\n",
      "‚úì Forecast plot saved\n",
      "\n",
      "=== ANOMALY DETECTION ===\n",
      "‚úì Detected 11 anomalies\n",
      "Anomaly threshold: ¬±3 std devs\n",
      "Anomaly dates: [Timestamp('2018-05-19 00:00:00'), Timestamp('2019-07-24 00:00:00'), Timestamp('2019-08-05 00:00:00'), Timestamp('2020-02-28 00:00:00'), Timestamp('2020-07-16 00:00:00')]\n",
      "‚úì Anomaly detection plots saved\n",
      "\n",
      "============================================================\n",
      "           ENERGY CONSUMPTION ANALYSIS SUMMARY\n",
      "============================================================\n",
      "\n",
      "üìä DATASET OVERVIEW:\n",
      "   ‚Ä¢ File: DRIVER={ODBC Driver 17 for SQL Server};SERVER=ge-prd.database.windows.net;DATABASE=GreenEnergy_DBP;UID=Nalinpgdde@chndsrnvsgmail.onmicrosoft.com;PWD=Neilapple7#;Authentication=ActiveDirectoryPassword;Encrypt=yes;TrustServerCertificate=no;\n",
      "   ‚Ä¢ Target variable: daily_avg_kwh\n",
      "   ‚Ä¢ Total observations: 1826\n",
      "   ‚Ä¢ Date range: 2018-01-01 00:00:00 to 2022-12-31 00:00:00\n",
      "   ‚Ä¢ Data frequency: Hourly\n",
      "\n",
      "üîç STATIONARITY ANALYSIS:\n",
      "   ‚Ä¢ Differencing order (d): 0\n",
      "   ‚Ä¢ Original series used\n",
      "\n",
      "üìà MODEL DETAILS:\n",
      "   ‚Ä¢ Model: ARIMA(5, 0, 5)\n",
      "   ‚Ä¢ Training observations: 1460\n",
      "   ‚Ä¢ Test observations: 366\n",
      "   ‚Ä¢ AIC: -2476.30\n",
      "\n",
      "üìä MODEL PERFORMANCE:\n",
      "   ‚Ä¢ RMSE: 0.1042\n",
      "   ‚Ä¢ MAE: 0.0814\n",
      "   ‚Ä¢ R¬≤: -0.0027\n",
      "   ‚Ä¢ MAPE: 5.43%\n",
      "\n",
      "üö® ANOMALY DETECTION:\n",
      "   ‚Ä¢ Anomalies detected: 11\n",
      "   ‚Ä¢ Detection method: ¬±3 standard deviations\n",
      "   ‚Ä¢ Anomaly percentage: 0.60%\n",
      "\n",
      "üìÅ OUTPUT FILES:\n",
      "   ‚Ä¢ All plots saved to: energy_analysis_output/\n",
      "   ‚Ä¢ Generated plots: original_timeseries.png, forecast_results.png, anomaly_detection.png\n",
      "\n",
      "üí° RECOMMENDATIONS:\n",
      "   ‚Ä¢ Evaluate SARIMA models for seasonal patterns\n",
      "   ‚Ä¢ Consider external variables (weather, holidays) for improved accuracy\n",
      "   ‚Ä¢ Implement real-time monitoring using the anomaly detection framework\n",
      "\n",
      "============================================================\n",
      "\n",
      "‚úÖ Analysis complete!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.stats.diagnostic import acorr_ljungbox\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from scipy import stats\n",
    "import warnings\n",
    "import os\n",
    "import pyodbc\n",
    "from datetime import datetime\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "class EnergyConsumptionAnalyzer:\n",
    "    \"\"\"\n",
    "    A comprehensive class for energy consumption time series analysis using ARIMA models.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, filepath, sep='\\t', datetime_col='datetime', target_col='avg_consumption_kwh'):\n",
    "        self.filepath = filepath\n",
    "        self.sep = sep\n",
    "        self.datetime_col = datetime_col\n",
    "        self.target_col = target_col\n",
    "        self.df = None\n",
    "        self.ts = None\n",
    "        self.ts_for_modeling = None\n",
    "        self.model_fit = None\n",
    "        self.d_order = 0\n",
    "        self.p_order = 0\n",
    "        self.q_order = 0\n",
    "        self.train_ts = None\n",
    "        self.test_ts = None\n",
    "        self.predictions = None\n",
    "        self.anomalies = pd.Series([], dtype='float64')\n",
    "        self.metrics = {}\n",
    "        \n",
    "        # Constants\n",
    "        self.MIN_OBS_ADF = 5\n",
    "        self.MIN_OBS_ARIMA = 24 * 7  # One week of hourly data\n",
    "        self.MIN_OBS_TRAIN_TEST = 2\n",
    "        self.ANOMALY_THRESHOLD = 3  # Standard deviations for anomaly detection\n",
    "        \n",
    "        # Create output directory\n",
    "        self.output_dir = 'energy_analysis_output'\n",
    "        os.makedirs(self.output_dir, exist_ok=True)\n",
    "        \n",
    "    def load_and_preprocess_data(self):\n",
    "        \"\"\"Load and preprocess the energy consumption data from SQL Server view.\"\"\"\n",
    "        try:\n",
    "            # Connect to SQL Server\n",
    "            conn = pyodbc.connect(self.filepath)  # self.filepath is now conn_str\n",
    "            query = \"SELECT * FROM [dbo].[vw_daily_consumption_summary]\"\n",
    "            self.df = pd.read_sql(query, conn)\n",
    "            conn.close()\n",
    "    \n",
    "            print(\"‚úì Data loaded successfully from database view\")\n",
    "            print(f\"Dataset shape: {self.df.shape}\")\n",
    "            print(f\"Columns: {list(self.df.columns)}\")\n",
    "    \n",
    "            # Handle datetime column\n",
    "            if self.datetime_col not in self.df.columns:\n",
    "                print(f\"‚ùå '{self.datetime_col}' column not found.\")\n",
    "                print(f\"Available columns: {list(self.df.columns)}\")\n",
    "                return False\n",
    "    \n",
    "            self.df[self.datetime_col] = pd.to_datetime(self.df[self.datetime_col])\n",
    "            self.df = self.df.set_index(self.datetime_col).sort_index()\n",
    "    \n",
    "            # Handle target column\n",
    "            if self.target_col not in self.df.columns:\n",
    "                print(f\"‚ùå '{self.target_col}' column not found.\")\n",
    "                numeric_cols = self.df.select_dtypes(include=np.number).columns\n",
    "                if len(numeric_cols) > 0:\n",
    "                    self.target_col = numeric_cols[0]\n",
    "                    print(f\"‚úì Using '{self.target_col}' as target variable\")\n",
    "                else:\n",
    "                    print(\"‚ùå No numeric columns found\")\n",
    "                    return False\n",
    "    \n",
    "            # Create time series\n",
    "            self.ts = self.df[self.target_col].copy()\n",
    "    \n",
    "            # Handle missing values\n",
    "            missing_count = self.ts.isnull().sum()\n",
    "            if missing_count > 0:\n",
    "                print(f\"‚ö†Ô∏è  Found {missing_count} missing values. Filling with forward/backward fill.\")\n",
    "                self.ts = self.ts.ffill().bfill()\n",
    "    \n",
    "            # Ensure consistent frequency\n",
    "            self.ts = self.ts.asfreq('D').ffill().bfill()\n",
    "    \n",
    "            print(f\"‚úì Time series preprocessed: {len(self.ts)} observations\")\n",
    "            print(f\"Date range: {self.ts.index.min()} to {self.ts.index.max()}\")\n",
    "    \n",
    "            return True\n",
    "    \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error loading data from database: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def check_stationarity(self, timeseries, title=\"Series\"):\n",
    "        \"\"\"Enhanced stationarity check with better error handling.\"\"\"\n",
    "        print(f'\\n--- Stationarity Check: {title} ---')\n",
    "        \n",
    "        nobs = len(timeseries)\n",
    "        if nobs < self.MIN_OBS_ADF:\n",
    "            print(f\"‚ö†Ô∏è  Not enough observations ({nobs}) for reliable ADF test\")\n",
    "            return False\n",
    "            \n",
    "        try:\n",
    "            # Try with automatic lag selection first\n",
    "            dftest = adfuller(timeseries, autolag='AIC')\n",
    "        except ValueError:\n",
    "            # Fallback to manual lag selection\n",
    "            safe_max_lag = max(1, int(nobs / 3))\n",
    "            try:\n",
    "                dftest = adfuller(timeseries, autolag=None, maxlag=safe_max_lag)\n",
    "            except ValueError as e:\n",
    "                print(f\"‚ùå ADF test failed: {e}\")\n",
    "                return False\n",
    "        \n",
    "        # Format results\n",
    "        results = pd.Series(dftest[0:4], \n",
    "                           index=['Test Statistic', 'p-value', 'Lags Used', 'Observations Used'])\n",
    "        \n",
    "        for key, value in dftest[4].items():\n",
    "            results[f'Critical Value ({key})'] = value\n",
    "            \n",
    "        print(results.to_string())\n",
    "        \n",
    "        is_stationary = dftest[1] <= 0.05\n",
    "        conclusion = \"‚úì Stationary\" if is_stationary else \"‚ùå Non-Stationary\"\n",
    "        print(f\"Conclusion: {conclusion}\")\n",
    "        \n",
    "        return is_stationary\n",
    "    \n",
    "    def perform_seasonal_decomposition(self):\n",
    "        \"\"\"Perform seasonal decomposition to understand data patterns.\"\"\"\n",
    "        if len(self.ts) < 2 * 24:  # Need at least 2 days for daily seasonality\n",
    "            print(\"‚ö†Ô∏è  Insufficient data for seasonal decomposition\")\n",
    "            return\n",
    "            \n",
    "        try:\n",
    "            print(\"\\n--- Seasonal Decomposition ---\")\n",
    "            decomposition = seasonal_decompose(self.ts, model='additive', period=24)\n",
    "            \n",
    "            fig, axes = plt.subplots(4, 1, figsize=(15, 12))\n",
    "            decomposition.observed.plot(ax=axes[0], title='Original')\n",
    "            decomposition.trend.plot(ax=axes[1], title='Trend')\n",
    "            decomposition.seasonal.plot(ax=axes[2], title='Seasonal')\n",
    "            decomposition.resid.plot(ax=axes[3], title='Residual')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f'{self.output_dir}/seasonal_decomposition.png', dpi=300, bbox_inches='tight')\n",
    "            plt.close()\n",
    "            \n",
    "            print(\"‚úì Seasonal decomposition plot saved\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Seasonal decomposition failed: {e}\")\n",
    "    \n",
    "    def determine_stationarity_and_differencing(self):\n",
    "        \"\"\"Determine if differencing is needed and apply it.\"\"\"\n",
    "        print(\"\\n=== STATIONARITY ANALYSIS ===\")\n",
    "        \n",
    "        # Check original series\n",
    "        is_stationary = self.check_stationarity(self.ts, \"Original Series\")\n",
    "        \n",
    "        if is_stationary:\n",
    "            self.ts_for_modeling = self.ts\n",
    "            self.d_order = 0\n",
    "            print(\"‚úì Original series is stationary\")\n",
    "        else:\n",
    "            print(\"\\nüîÑ Applying first-order differencing...\")\n",
    "            ts_diff = self.ts.diff().dropna()\n",
    "            \n",
    "            if len(ts_diff) < self.MIN_OBS_ADF:\n",
    "                print(\"‚ö†Ô∏è  Differenced series too short for reliable analysis\")\n",
    "                self.ts_for_modeling = self.ts\n",
    "                self.d_order = 0\n",
    "            else:\n",
    "                is_diff_stationary = self.check_stationarity(ts_diff, \"Differenced Series\")\n",
    "                \n",
    "                if is_diff_stationary:\n",
    "                    self.ts_for_modeling = ts_diff\n",
    "                    self.d_order = 1\n",
    "                    print(\"‚úì Differenced series is stationary\")\n",
    "                else:\n",
    "                    print(\"‚ö†Ô∏è  Differenced series still non-stationary, using d=1 anyway\")\n",
    "                    self.ts_for_modeling = ts_diff\n",
    "                    self.d_order = 1\n",
    "    \n",
    "    def plot_acf_pacf(self):\n",
    "        \"\"\"Plot ACF and PACF for parameter determination.\"\"\"\n",
    "        if len(self.ts_for_modeling) < self.MIN_OBS_ADF:\n",
    "            print(\"‚ö†Ô∏è  Insufficient data for ACF/PACF plots\")\n",
    "            return\n",
    "            \n",
    "        print(\"\\n--- ACF/PACF Analysis ---\")\n",
    "        \n",
    "        # Calculate appropriate number of lags\n",
    "        max_lags = min(len(self.ts_for_modeling) // 2 - 1, 48)  # Up to 48 for daily patterns\n",
    "        max_lags = max(1, max_lags)\n",
    "        \n",
    "        try:\n",
    "            fig, axes = plt.subplots(2, 1, figsize=(15, 10))\n",
    "            \n",
    "            plot_acf(self.ts_for_modeling, lags=max_lags, ax=axes[0], \n",
    "                    title=f'ACF - {self.target_col}')\n",
    "            plot_pacf(self.ts_for_modeling, lags=max_lags, ax=axes[1], \n",
    "                     title=f'PACF - {self.target_col}')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f'{self.output_dir}/acf_pacf_plots.png', dpi=300, bbox_inches='tight')\n",
    "            plt.close()\n",
    "            \n",
    "            print(\"‚úì ACF/PACF plots saved\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå ACF/PACF plotting failed: {e}\")\n",
    "    \n",
    "    def split_data(self, train_ratio=0.8):\n",
    "        \"\"\"Split data into training and testing sets.\"\"\"\n",
    "        if len(self.ts) < self.MIN_OBS_TRAIN_TEST:\n",
    "            print(f\"‚ùå Insufficient data for train-test split: {len(self.ts)} observations\")\n",
    "            return False\n",
    "            \n",
    "        train_size = max(1, int(len(self.ts) * train_ratio))\n",
    "        \n",
    "        # Ensure at least one observation in test set\n",
    "        if len(self.ts) - train_size < 1 and len(self.ts) > 1:\n",
    "            train_size = len(self.ts) - 1\n",
    "            \n",
    "        self.train_ts = self.ts[:train_size]\n",
    "        self.test_ts = self.ts[train_size:]\n",
    "        \n",
    "        print(f\"\\n--- Data Split ---\")\n",
    "        print(f\"Training set: {len(self.train_ts)} observations\")\n",
    "        print(f\"Test set: {len(self.test_ts)} observations\")\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def fit_arima_model(self):\n",
    "        \"\"\"Fit ARIMA model with automatic parameter selection.\"\"\"\n",
    "        if len(self.train_ts) < self.MIN_OBS_ARIMA:\n",
    "            print(f\"‚ö†Ô∏è  Training set too small for reliable ARIMA: {len(self.train_ts)} observations\")\n",
    "            print(f\"Minimum recommended: {self.MIN_OBS_ARIMA}\")\n",
    "            \n",
    "        print(f\"\\n=== ARIMA MODEL FITTING ===\")\n",
    "        \n",
    "        # Try different parameter combinations\n",
    "        param_combinations = [\n",
    "            (1, self.d_order, 1),\n",
    "            (2, self.d_order, 2),\n",
    "            (5, self.d_order, 5),\n",
    "            (1, self.d_order, 2),\n",
    "            (2, self.d_order, 1),\n",
    "        ]\n",
    "        \n",
    "        best_aic = float('inf')\n",
    "        best_params = None\n",
    "        \n",
    "        for p, d, q in param_combinations:\n",
    "            try:\n",
    "                print(f\"Trying ARIMA({p},{d},{q})...\")\n",
    "                model = ARIMA(self.train_ts, order=(p, d, q))\n",
    "                model_fit = model.fit()\n",
    "                \n",
    "                aic = model_fit.aic\n",
    "                print(f\"  AIC: {aic:.2f}\")\n",
    "                \n",
    "                if aic < best_aic:\n",
    "                    best_aic = aic\n",
    "                    best_params = (p, d, q)\n",
    "                    self.model_fit = model_fit\n",
    "                    self.p_order, _, self.q_order = p, d, q\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"  ‚ùå Failed: {e}\")\n",
    "                continue\n",
    "        \n",
    "        if self.model_fit is not None:\n",
    "            print(f\"\\n‚úì Best model: ARIMA{best_params} (AIC: {best_aic:.2f})\")\n",
    "            print(\"\\nModel Summary:\")\n",
    "            print(self.model_fit.summary())\n",
    "            \n",
    "            # Diagnostic tests\n",
    "            self.perform_diagnostic_tests()\n",
    "            \n",
    "            return True\n",
    "        else:\n",
    "            print(\"‚ùå No ARIMA model could be fitted\")\n",
    "            return False\n",
    "    \n",
    "    def perform_diagnostic_tests(self):\n",
    "        \"\"\"Perform diagnostic tests on the fitted model.\"\"\"\n",
    "        print(\"\\n--- Model Diagnostics ---\")\n",
    "        \n",
    "        try:\n",
    "            # Ljung-Box test for residual autocorrelation\n",
    "            residuals = self.model_fit.resid\n",
    "            lb_test = acorr_ljungbox(residuals, lags=10, return_df=True)\n",
    "            \n",
    "            significant_lags = lb_test[lb_test['lb_pvalue'] < 0.05]\n",
    "            \n",
    "            if len(significant_lags) == 0:\n",
    "                print(\"‚úì Ljung-Box test: No significant autocorrelation in residuals\")\n",
    "            else:\n",
    "                print(f\"‚ö†Ô∏è  Ljung-Box test: Significant autocorrelation detected at {len(significant_lags)} lag(s)\")\n",
    "                \n",
    "            # Normality test for residuals\n",
    "            _, p_value = stats.jarque_bera(residuals)\n",
    "            if p_value > 0.05:\n",
    "                print(\"‚úì Jarque-Bera test: Residuals are normally distributed\")\n",
    "            else:\n",
    "                print(\"‚ö†Ô∏è  Jarque-Bera test: Residuals may not be normally distributed\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Diagnostic tests failed: {e}\")\n",
    "    \n",
    "    def forecast_and_evaluate(self):\n",
    "        \"\"\"Make forecasts and evaluate model performance.\"\"\"\n",
    "        if self.model_fit is None or len(self.test_ts) == 0:\n",
    "            print(\"‚ö†Ô∏è  Cannot perform forecasting: model not fitted or no test data\")\n",
    "            return\n",
    "            \n",
    "        print(\"\\n=== MODEL EVALUATION ===\")\n",
    "        \n",
    "        try:\n",
    "            # Make forecasts\n",
    "            forecast_steps = len(self.test_ts)\n",
    "            self.predictions = self.model_fit.forecast(steps=forecast_steps)\n",
    "            self.predictions.index = self.test_ts.index\n",
    "            \n",
    "            # Calculate metrics\n",
    "            rmse = np.sqrt(mean_squared_error(self.test_ts, self.predictions))\n",
    "            mae = mean_absolute_error(self.test_ts, self.predictions)\n",
    "            r2 = r2_score(self.test_ts, self.predictions)\n",
    "            \n",
    "            # Calculate MAPE (Mean Absolute Percentage Error)\n",
    "            mape = np.mean(np.abs((self.test_ts - self.predictions) / self.test_ts)) * 100\n",
    "            \n",
    "            self.metrics = {\n",
    "                'RMSE': rmse,\n",
    "                'MAE': mae,\n",
    "                'R¬≤': r2,\n",
    "                'MAPE': mape\n",
    "            }\n",
    "            \n",
    "            print(f\"Test RMSE: {rmse:.4f}\")\n",
    "            print(f\"Test MAE: {mae:.4f}\")\n",
    "            print(f\"Test R¬≤: {r2:.4f}\")\n",
    "            print(f\"Test MAPE: {mape:.2f}%\")\n",
    "            \n",
    "            # Plot forecast results\n",
    "            self.plot_forecast_results()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Forecasting failed: {e}\")\n",
    "    \n",
    "    def plot_forecast_results(self):\n",
    "        \"\"\"Plot actual vs predicted values.\"\"\"\n",
    "        plt.figure(figsize=(15, 8))\n",
    "        \n",
    "        # Plot training data\n",
    "        plt.plot(self.train_ts.index, self.train_ts, \n",
    "                label='Training Data', color='gray', alpha=0.7)\n",
    "        \n",
    "        # Plot test data\n",
    "        plt.plot(self.test_ts.index, self.test_ts, \n",
    "                label='Actual (Test)', color='blue', linewidth=2)\n",
    "        \n",
    "        # Plot predictions\n",
    "        plt.plot(self.predictions.index, self.predictions, \n",
    "                label='Forecast', color='red', linewidth=2, linestyle='--')\n",
    "        \n",
    "        plt.title(f'Energy Consumption Forecasting - {self.target_col}')\n",
    "        plt.xlabel('Date')\n",
    "        plt.ylabel(self.target_col)\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{self.output_dir}/forecast_results.png', dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        print(\"‚úì Forecast plot saved\")\n",
    "    \n",
    "    def detect_anomalies(self):\n",
    "        \"\"\"Detect anomalies using residual analysis.\"\"\"\n",
    "        if self.model_fit is None:\n",
    "            print(\"‚ö†Ô∏è  Cannot detect anomalies: model not fitted\")\n",
    "            return\n",
    "            \n",
    "        print(\"\\n=== ANOMALY DETECTION ===\")\n",
    "        \n",
    "        try:\n",
    "            # Get predictions for full series\n",
    "            full_predictions = self.model_fit.predict(start=0, end=len(self.ts)-1, typ='levels')\n",
    "            full_predictions.index = self.ts.index\n",
    "            \n",
    "            # Calculate residuals\n",
    "            residuals = self.ts - full_predictions\n",
    "            \n",
    "            if len(residuals) < 2:\n",
    "                print(\"‚ö†Ô∏è  Insufficient data for anomaly detection\")\n",
    "                return\n",
    "                \n",
    "            # Define anomaly thresholds\n",
    "            residual_mean = residuals.mean()\n",
    "            residual_std = residuals.std()\n",
    "            \n",
    "            upper_threshold = residual_mean + self.ANOMALY_THRESHOLD * residual_std\n",
    "            lower_threshold = residual_mean - self.ANOMALY_THRESHOLD * residual_std\n",
    "            \n",
    "            # Identify anomalies\n",
    "            self.anomalies = residuals[\n",
    "                (residuals > upper_threshold) | (residuals < lower_threshold)\n",
    "            ]\n",
    "            \n",
    "            print(f\"‚úì Detected {len(self.anomalies)} anomalies\")\n",
    "            print(f\"Anomaly threshold: ¬±{self.ANOMALY_THRESHOLD} std devs\")\n",
    "            \n",
    "            if len(self.anomalies) > 0:\n",
    "                print(f\"Anomaly dates: {self.anomalies.index.tolist()[:5]}\")  # Show first 5\n",
    "                \n",
    "            # Plot anomaly results\n",
    "            self.plot_anomaly_results(residuals, upper_threshold, lower_threshold)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Anomaly detection failed: {e}\")\n",
    "    \n",
    "    def plot_anomaly_results(self, residuals, upper_threshold, lower_threshold):\n",
    "        \"\"\"Plot residuals and detected anomalies.\"\"\"\n",
    "        fig, axes = plt.subplots(2, 1, figsize=(15, 12))\n",
    "        \n",
    "        # Plot 1: Residuals with thresholds\n",
    "        axes[0].plot(residuals.index, residuals, label='Residuals', color='gray', alpha=0.7)\n",
    "        axes[0].axhline(y=0, color='green', linestyle='--', label='Mean Residual')\n",
    "        axes[0].axhline(y=upper_threshold, color='red', linestyle='-', label='Upper Threshold')\n",
    "        axes[0].axhline(y=lower_threshold, color='red', linestyle='-', label='Lower Threshold')\n",
    "        axes[0].scatter(self.anomalies.index, self.anomalies, \n",
    "                       color='red', s=50, zorder=5, label='Anomalies')\n",
    "        axes[0].set_title('Residuals and Anomaly Detection')\n",
    "        axes[0].set_ylabel('Residual')\n",
    "        axes[0].legend()\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot 2: Original series with anomalies\n",
    "        axes[1].plot(self.ts.index, self.ts, label=f'Actual {self.target_col}', color='blue')\n",
    "        if len(self.anomalies) > 0:\n",
    "            axes[1].scatter(self.anomalies.index, self.ts.loc[self.anomalies.index], \n",
    "                           color='red', s=100, zorder=5, label='Anomalies')\n",
    "        axes[1].set_title('Energy Consumption with Detected Anomalies')\n",
    "        axes[1].set_xlabel('Date')\n",
    "        axes[1].set_ylabel(self.target_col)\n",
    "        axes[1].legend()\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{self.output_dir}/anomaly_detection.png', dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        print(\"‚úì Anomaly detection plots saved\")\n",
    "    \n",
    "    def plot_original_series(self):\n",
    "        \"\"\"Plot the original time series.\"\"\"\n",
    "        plt.figure(figsize=(15, 6))\n",
    "        plt.plot(self.ts.index, self.ts, label=f'Original {self.target_col}')\n",
    "        plt.title(f'Energy Consumption Over Time - {self.target_col}')\n",
    "        plt.xlabel('Date')\n",
    "        plt.ylabel(self.target_col)\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{self.output_dir}/original_timeseries.png', dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        print(\"‚úì Original series plot saved\")\n",
    "    \n",
    "    def generate_summary_report(self):\n",
    "        \"\"\"Generate a comprehensive summary report.\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"           ENERGY CONSUMPTION ANALYSIS SUMMARY\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        print(f\"\\nüìä DATASET OVERVIEW:\")\n",
    "        print(f\"   ‚Ä¢ File: {self.filepath}\")\n",
    "        print(f\"   ‚Ä¢ Target variable: {self.target_col}\")\n",
    "        print(f\"   ‚Ä¢ Total observations: {len(self.ts)}\")\n",
    "        print(f\"   ‚Ä¢ Date range: {self.ts.index.min()} to {self.ts.index.max()}\")\n",
    "        print(f\"   ‚Ä¢ Data frequency: Hourly\")\n",
    "        \n",
    "        print(f\"\\nüîç STATIONARITY ANALYSIS:\")\n",
    "        print(f\"   ‚Ä¢ Differencing order (d): {self.d_order}\")\n",
    "        print(f\"   ‚Ä¢ {'Applied first-order differencing' if self.d_order == 1 else 'Original series used'}\")\n",
    "        \n",
    "        if self.model_fit is not None:\n",
    "            print(f\"\\nüìà MODEL DETAILS:\")\n",
    "            print(f\"   ‚Ä¢ Model: ARIMA({self.p_order}, {self.d_order}, {self.q_order})\")\n",
    "            print(f\"   ‚Ä¢ Training observations: {len(self.train_ts)}\")\n",
    "            print(f\"   ‚Ä¢ Test observations: {len(self.test_ts)}\")\n",
    "            print(f\"   ‚Ä¢ AIC: {self.model_fit.aic:.2f}\")\n",
    "            \n",
    "            if self.metrics:\n",
    "                print(f\"\\nüìä MODEL PERFORMANCE:\")\n",
    "                for metric, value in self.metrics.items():\n",
    "                    if metric == 'MAPE':\n",
    "                        print(f\"   ‚Ä¢ {metric}: {value:.2f}%\")\n",
    "                    else:\n",
    "                        print(f\"   ‚Ä¢ {metric}: {value:.4f}\")\n",
    "        \n",
    "        print(f\"\\nüö® ANOMALY DETECTION:\")\n",
    "        print(f\"   ‚Ä¢ Anomalies detected: {len(self.anomalies)}\")\n",
    "        print(f\"   ‚Ä¢ Detection method: ¬±{self.ANOMALY_THRESHOLD} standard deviations\")\n",
    "        \n",
    "        if len(self.anomalies) > 0:\n",
    "            print(f\"   ‚Ä¢ Anomaly percentage: {len(self.anomalies)/len(self.ts)*100:.2f}%\")\n",
    "        \n",
    "        print(f\"\\nüìÅ OUTPUT FILES:\")\n",
    "        print(f\"   ‚Ä¢ All plots saved to: {self.output_dir}/\")\n",
    "        print(f\"   ‚Ä¢ Generated plots: original_timeseries.png, forecast_results.png, anomaly_detection.png\")\n",
    "        \n",
    "        print(f\"\\nüí° RECOMMENDATIONS:\")\n",
    "        if len(self.ts) < self.MIN_OBS_ARIMA:\n",
    "            print(\"   ‚Ä¢ Consider collecting more data for robust ARIMA modeling\")\n",
    "        print(\"   ‚Ä¢ Evaluate SARIMA models for seasonal patterns\")\n",
    "        print(\"   ‚Ä¢ Consider external variables (weather, holidays) for improved accuracy\")\n",
    "        print(\"   ‚Ä¢ Implement real-time monitoring using the anomaly detection framework\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "    \n",
    "    def run_full_analysis(self):\n",
    "        \"\"\"Run the complete analysis pipeline.\"\"\"\n",
    "        print(\"üöÄ Starting Energy Consumption Analysis...\")\n",
    "        \n",
    "        # Step 1: Load and preprocess data\n",
    "        if not self.load_and_preprocess_data():\n",
    "            return\n",
    "        \n",
    "        # Step 2: Plot original series\n",
    "        self.plot_original_series()\n",
    "        \n",
    "        # Step 3: Seasonal decomposition\n",
    "        self.perform_seasonal_decomposition()\n",
    "        \n",
    "        # Step 4: Determine stationarity\n",
    "        self.determine_stationarity_and_differencing()\n",
    "        \n",
    "        # Step 5: ACF/PACF analysis\n",
    "        self.plot_acf_pacf()\n",
    "        \n",
    "        # Step 6: Split data\n",
    "        if not self.split_data():\n",
    "            return\n",
    "        \n",
    "        # Step 7: Fit ARIMA model\n",
    "        if not self.fit_arima_model():\n",
    "            return\n",
    "        \n",
    "        # Step 8: Forecast and evaluate\n",
    "        self.forecast_and_evaluate()\n",
    "        \n",
    "        # Step 9: Detect anomalies\n",
    "        self.detect_anomalies()\n",
    "        \n",
    "        # Step 10: Generate summary\n",
    "        self.generate_summary_report()\n",
    "        \n",
    "        print(\"\\n‚úÖ Analysis complete!\")\n",
    "\n",
    "# Usage example\n",
    "if __name__ == \"__main__\":\n",
    "    conn_str = (\n",
    "        \"DRIVER={ODBC Driver 17 for SQL Server};\"\n",
    "        \"SERVER=ge-prd.database.windows.net;\"\n",
    "        \"DATABASE=GreenEnergy_DBP;\"\n",
    "        \"UID=Nalinpgdde@chndsrnvsgmail.onmicrosoft.com;\"\n",
    "        \"PWD=Neilapple7#;\"\n",
    "        \"Authentication=ActiveDirectoryPassword;\"\n",
    "        \"Encrypt=yes;\"\n",
    "        \"TrustServerCertificate=no;\"\n",
    "    )\n",
    "    analyzer = EnergyConsumptionAnalyzer(\n",
    "    filepath=conn_str, \n",
    "    datetime_col='day',          # üëà use 'day' instead of 'datetime'\n",
    "    target_col='daily_avg_kwh'   # üëà use 'daily_avg_kwh' instead of 'avg_consumption_kwh'\n",
    "    )\n",
    "    analyzer.run_full_analysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0271944-2770-40a4-949e-544e1a086747",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
